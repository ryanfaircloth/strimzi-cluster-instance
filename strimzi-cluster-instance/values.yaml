# Default values for strimzi-cluster-instance.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# This is to override the chart name.
fullnameOverride: ""
nameOverride: ""

# Node pools configuration
nodePools:
  - name: broker
    roles:
      - broker
    replicas: 3
    additionalAnnotations:
    strimzi.io/next-node-ids: "[0-99]"
    template:
      pod:
        metadata:
          labels:
            app.kubernetes.io/component: broker
        affinity:
          nodeAffinity: {}
          podAffinity: {}
          podAntiAffinity: {}
        tolerations: []
        topologySpreadConstraints: []
        priorityClassName: ""
        tmpDirSizeLimit: ""
        volumes: []
    storage:
      type: jbod
      volumes:
        - id: 0
          type: ephemeral
  #   storage:
  #     type: jbod
  #     volumes:
  #       - id: 0
  #         type: persistent-claim
  #         size: 64Gi
  #         # Indicates that this directory will be used to store Kraft metadata log
  #         kraftMetadata: shared
  #         deleteClaim: false
  #       - id: 1
  #         type: persistent-claim
  #         size: 64Gi
  #         deleteClaim: false
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      # limits:
      #   memory: "4Gi"
      #   cpu: "2000m"
  
  - name: controller
    roles:
      - controller
    replicas: 3
    additionalAnnotations:
        strimzi.io/next-node-ids: "[100-999]"
    template:
      pod:
        metadata:
          labels:
            app.kubernetes.io/component: controller
        affinity:
          nodeAffinity: {}
          podAffinity: {}
          podAntiAffinity: {}
        tolerations: []
        topologySpreadConstraints: []
        priorityClassName: ""
        tmpDirSizeLimit: ""
        volumes: []
    storage:
      type: jbod
      volumes:
        - id: 0
          type: ephemeral
          sizeLimit: 64Gi
          kraftMetadata: shared  
  #   storage:
  #     type: jbod
  #     volumes:
  #       - id: 0
  #         type: persistent-claim
  #         size: 64Gi
  #         # Indicates that this directory will be used to store Kraft metadata log
  #         kraftMetadata: shared
  #         deleteClaim: false
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      # limits:
      #   memory: "2Gi"
      #   cpu: "1000m"

# Kafka CR settings
kafka:
  additionalAnnotations: {}
  additionalLabels: {}
  brokerRackInitImage: null
  
  config:
    auto.create.topics.enable: true
    # background threads include replication for very high throughput increase based on available CPU cores and usage default is 10
    # background.threads: 20
    default.replication.factor: 2
    # For storage where storage is thin provisioned enable
    # log.preallocate: true
    min.insync.replicas: 2
    # num.io.threads increase based on available CPU cores and usage default is 8
    # num.io.threads: 20
    # num.network.threads increase based on available CPU cores and usage default is 3
    # num.network.threads: 6
    # num.recovery.threads.per.data.dir: 2  # default is 1 set to number of cores in brokers
    # num.recovery.threads.per.data.dir: 1
    # num.replica.fetchers increase based on available CPU cores and usage default is 1
    # num.replica.fetchers: 10
    offsets.topic.replication.factor: 3
    replica.selector.class: org.apache.kafka.common.replica.RackAwareReplicaSelector
    # replica.socket.receive.buffer.bytes: 65536 increased for higher replication throughput based on memory
    # replica.socket.receive.buffer.bytes: 1000000
    # socket.receive.buffer.bytes: 102400 default setting to -1 to use OS default
    socket.receive.buffer.bytes: -1
    # socket.request.max.bytes: 104857600 default setting to 100MB probably should not change
    # socket.send.buffer.bytes: 102400 default setting to -1 to use OS default
    socket.send.buffer.bytes: -1
    transaction.state.log.min.isr: 2
    transaction.state.log.replication.factor: 3
  jmxOptions: {}
  listeners:
    - name: int4plain
      port: 9092
      type: internal
      tls: false      
    - name: int4tls
      port: 9093
      type: internal
      tls: true
  additionalListeners: # []
    # - name: ext4tls
    #   port: 9094
    #   type: ingress
    #   tls: true
    #   configuration:
    #     bootstrap:
    #       host: kafka-dev-bootstrap.domain.local  
    #     brokerCertChainAndKey:
    #       secretName: kafka-dev-sci-external
    #       certificate: tls.crt
    #       key: tls.key  
    #     hostTemplate: broker-{nodeId}.domain.local              
    #   additionalBrokerAnnotations: {}
    #   additionalBrokerLabels: {}

    - name: ext4tls
      port: 9094
      type: gateway
      tls: true
      generateTLSRoute: true
      configuration:
        bootstrap:
          host: kafka-dev-bootstrap.domain.local  
        brokerCertChainAndKey:
          secretName: kafka-dev-sci-external
          certificate: tls.crt
          key: tls.key
        createBootstrapService: true
        hostTemplate: broker-{nodeId}.domain.local  
      additionalBrokerAnnotations:
        external-dns.alpha.kubernetes.io/hostname: broker-{nodeId}.domain.local  
      additionalBrokerLabels: {}

  metadataVersion: null
  rack:
    topologyKey: topology.kubernetes.io/zone

  cruiseControl:
    brokerCapacity:
      inboundNetwork: 15728640KiB/s
      outboundNetwork: 15728640KiB/s
    config: {}
    template:
      pod:
        metadata:
          annotations:
            app.kubernetes.io/component: cruise-control
        affinity: {}
        tolerations: []
        topologySpreadConstraints: []
        priorityClassName: ""
        tmpDirSizeLimit: ""
        volumes: []
    resources:
      requests:
        memory: "1Gi"
        cpu: "1"
      # limits:
      #   memory: "2Gi"
      #   cpu: "1"
  entityOperator:
    template:
      pod:
        metadata:
          annotations:
            app.kubernetes.io/component: entity-operator
        affinity: {}
        tolerations: []
        topologySpreadConstraints: []
        priorityClassName: ""
        tmpDirSizeLimit: ""
        volumes: []
    resources:
      requests:
        memory: "1Gi"
        cpu: "1"
      # limits:
      #   memory: "2Gi"
      #   cpu: "1"
    topicOperator:
      watchAnyNamespace: false
      # Set watchedNamespace to the namespace where the operator should watch for resources
      # If empty, the operator will use the release namespace
      watchedNamespace: ""
    userOperator:
      watchAnyNamespace: false
      # Set watchedNamespace to the namespace where the operator should watch for resources
      # If empty, the operator will use the release namespace
      watchedNamespace: ""
    version: null

kafkaRebalance:
  enabled: true
  additionalAnnotations: {}
  additionalLabels: {}
  skipHardGoalCheck: false
  goals:
    - NetworkInboundCapacityGoal
    - DiskCapacityGoal
    - RackAwareGoal
    - NetworkOutboundCapacityGoal
    - CpuCapacityGoal
    - ReplicaCapacityGoal

tlsRoutes:
  parentRefs:
    - group: gateway.networking.k8s.io
      kind: Gateway
      name: my-gateway
      namespace: strimzi
      sectionName: kafka-listener
certificates:
  listeners:
    internal:
      enabled: false
      issuerRef:
        name: 
        kind: ClusterIssuer
        group: cert-manager.io
      additionalAnnotations: {}
      additionalLabels: {}
      subject:
        organizations:
          - "{{ .Release.Namespace }}"
      dnsNames: []
    external:
      enabled: true
      issuerRef:
        name: 
        kind: ClusterIssuer
        group: cert-manager.io
      additionalAnnotations: {}
      additionalLabels: {}
      subject:
        organizations:
          - "{{ .Release.Namespace }}"
      dnsNames: []